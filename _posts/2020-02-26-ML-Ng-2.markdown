---
layout:     post
title:      "Êú∫Âô®Â≠¶‰π†-Âê¥ÊÅ©ËææÔºöÂ≠¶‰π†Á¨îËÆ∞ÂèäÊÄªÁªìÔºà2Ôºâ"
subtitle:   "Coursera | Machine Learning | Andrew Ng"
date:       2020-02-28
author:     "Thistledown"
header-img: "img/posts/post-bg-Coursera-ML-Ng.png"
catalog: true
mathjax: true
tags:
    - ML/DL
    - Course Learning
    - Python
---

> ËØæÁ®ãÂú∞ÂùÄÔºö[Êú∫Âô®Â≠¶‰π† \| Coursera](https://www.coursera.org/learn/machine-learning/)ÔºåÊú¨ÁØáÁ¨îËÆ∞‰∏∫ *Week 2* ‰∏≠ÁöÑÂÜÖÂÆπ

## 5. Multivariate Linear Regression

#### 5.1 Multiple Features

- Training set:
    <img src="https://i.loli.net/2020/02/28/hLjSDHW1CTYKdNA.png" alt="image.png" style="zoom: 67%;" />
- Notation:
    - $m$ = the number of training examples
    - $n$ = number of features
    - $x^{(i)}$ = input (features) of $i^{th}$ training example (an $n$-dimensional vector, $i = 1, 2, \cdots, m$).
    - $x_j^{(i)}$ = value of feature $j$ in $i^{th}$ training example (a number, $j = 1, 2, \cdots, n$).
- Hypothesis: $h_\theta(x) = \theta_0 + \theta_1 x_1 + \theta_2 x_2 + \cdots + \theta_n x_n$
    - Previously: $h_\theta(x) = \theta_0 + \theta_1x$
    - Now: $h_\theta(x) = \theta_0 + \theta_1 x_1 + \theta_2 x_2 + \theta_3 x_3 + \theta_4 x_4$
    - For convenience of notation, define $x_0 = 1 \ (x_0^{(i)}=1,\ i=1, 2, \cdots, m)$.
    - $x = [ \begin{matrix}x_0 & x_1 & x_2 & \cdots & x_n \end{matrix}]^T \in \mathbb{R^{n+1}}$
    - $\theta = [ \begin{matrix}\theta_0 & \theta_1 & \theta_2 & \cdots & \theta_n \end{matrix}]^T \in \mathbb{R^{n+1}}$
- <u>Multivariate Linear Regression</u>

$$\begin{align}
      h_\theta(x)&=\theta^Tx= \begin{bmatrix}\theta_0 \ \theta_1 \ \theta_2 \ \cdots \ \theta_n \end{bmatrix} \begin{bmatrix}x_0 \\ x_1 \\ x_2 \\ \cdots \\ x_n \end{bmatrix} 
        = \theta_0 x_0 + \theta_1 x_1 + \theta_2 x_2 + \cdots + \theta_n x_n
        \end{align}$$

#### 5.2 Gradient Descent for Multiple Variables

- Hypothesis: $\begin{align}
          h_\theta(x)&=\theta^Tx 
            = \theta_0 x_0 + \theta_1 x_1 + \theta_2 x_2 + \cdots + \theta_n x_n
            \end{align}$

- Parameters: $\theta_0, \theta_1, \dots, \theta_n$  ($\theta = \begin{bmatrix} \theta_0 & \theta_1 & \cdots & \theta_n \end{bmatrix}^T$)

- Cost function: $J(\theta_0, \theta_1, \dots, \theta_n)=\frac{1}{2m}\sum_{i=1}^{m}(h_\theta(x^{(i)})-y^{(i)})^2 $

- Gradient descent:

    $$\left.\begin{array}{l} \text{repeat until convergence}\ \{ \\ \qquad \theta_j := \theta_j-\alpha\frac{\partial}{\partial\theta_j}J(\theta)=\theta_j-\alpha\frac{1}{m}\sum_{i=1}^{m}(h_\theta(x^{(i)})-y^{(i)})\cdot x_j^{(i)} \\ \qquad (\text{simultaneously update for every } \theta_j \text{ for } j := 0\dots n) \\ \} \end{array}\right.$$
    
- The following image compares gradient descent with one variable to gradient descent with multiple variables:
![image.png](https://i.loli.net/2020/02/29/tJXVdIn4EsCbDkf.png)

#### 5.3 Gradient Descent in Practice I - Feature Scaling

- <u>Feature scaling</u>
    - Idea: make sure features are on similar scale.
    - Get every feature into approximately a $-1 \leq x_i \leq 1$ range.
- <u>Mean normalization</u>: replace $x_i$ with $x_i - u_i$ to make features have approximately zero mean.
    - $x_i:= \frac{x_i-u_i}{s_i}$: $u_i$ is the *average* of all the values for the feature $(i)$ and $s_i$ is the range of the values $(\text{max - min})$, or $s_i$ is the standard deviation.

#### 5.4 Gradient Descent in Practice II - Learning Rate

- **Debugging gradient descent.** Make a plot with *number of iterations* on the x-axis. Now plot the cost function, $J(\theta)$ over the number of iterations of gradient descent. if $J(\theta)$ ever increases, then you probably need to decrease $\alpha$.
- **Automatic convergence test.** Declare convergence if $j(\theta)$ decreases by less than $E$ in one iteration, where $E$ is some small value such as $10^{-3}$. However in practice it's difficult to choose this threshold value.
- **Make sure gradient descent is working correctly.**
    - For sufficient small $\alpha$, $J(\theta)$ should decrease on every iteration.
    - If $\alpha$ is too large: may not decrease on every iteration and thus may not converge.
    - if $\alpha$ is too small: slow convergence.

#### 5.5 Features and Polynomial Regression

- [*Example*] **Housing prices prediction**: $h_\theta(x)=\theta_0 + \theta_1 \times frontage + \theta_2 \times depth$
- **Polynomial regression**: We can *change the behavior or curve* of our hypothesis function by making it a quadratic, cubic or square root function (or any other form).

## 6. Computing Parameters Analytically

#### 6.1 Normal Equation

- <u>Normal equation</u>: Method to solve for $\theta$ analytically.

    - $\theta \in \mathbb{R^{n+1}}\qquad J(\theta_0,\theta_1,\dots,\theta_n)=\frac{1}{2m}\sum_{i=1}^{m}(h_\theta(x^{(i)})-y^{(i)})^2$
    - $\frac{\partial}{\partial\theta_j}J(\theta)=\cdots=0 \qquad (\text{for every }j)$
    - Solve for $\theta_0, \theta_1, \dots,\theta_n$

- $m$ examples $(x^{(1)},y^{(1)}), \dots,(x^{(m)}, y^{(m)})$; $n$ features.

    $$x^{(i)}=\begin{bmatrix} x_0^{(i)} \\ x_1^{(i)} \\ x_2^{(i)} \\ \vdots \\ x_n^{(i)}  \end{bmatrix} \in \mathbb{R^{n+1}}, \qquad  \mathop{x}\limits_{\text{(design matrix)}} = \begin{bmatrix} (x^{(1)})^T \\ (x^{(2)})^T \\ \vdots \\  (x^{(m)})^T \end{bmatrix}_{m\times(n+1)}$$

- $\theta=(X^TX)^{-1}X^Ty$ 

- The following is a comparison of gradient descent and the normal equation: 

| <u>Gradient Descent</u>            | <u>Normal Equation</u>                         |
| ---------------------------------- | ---------------------------------------------- |
| Need to choose $\alpha$.           | No need to choose $\alpha$.                    |
| Needs many iterations.             | Don't need to iterate.                         |
| $\text{O}(kn^2)$                   | $\text{O}(n^3)$, Need to compute $(X^TX)^{-1}$ |
| Works well even when $n$ is large. | Slow if $n$ is very large.                     |

#### 6.2 Normal Equation and Noninvertibility

- Normal equation: $\theta=(X^TX)^{-1}X^Ty$. But what if $X^TX$ is non-invertible (singular/degenerate)?
- If $X^TX$ is *noninvertible*, the common causes might be having:
    - Redundant features, where tow features are very closely related (i.e. they are linearly dependent).
    - Too many features (e.g. $m \leq n$). In this case, delete some features, or use regularization.

## Ex1: Linear Regressionüë®‚Äçüíª

#### Ex1.1 Linear regression with one variable

**Instruction:**
In this part of this exercise, you will implement linear regression with one variable to predict profits for a food truck. Suppose you are the CEO of a restaurant franchise and are considering different cities for opening a new outlet. The chain already has trucks in various cities and you have data for profits and populations from the cities.

**Code:**

```python
import numpy as np
import matplotlib.pyplot as plt
from matplotlib import cm
from mpl_toolkits.mplot3d import Axes3D

""" Part 1: Basic Function """
def computeCost(X, y, theta):
    '''
    Compute cost for linear regression
    :param X: input variables
    :param y: output variables
    :param theta: parameters
    :return: Cost function
    '''
    m = len(y)
    J = 0
    for xi, yi in zip(X, y):
        J = J + np.square(float(np.dot(xi, theta)) - yi)
    return J / (2 * m)

def gradientDescent(X, y, theta, alpha, num_iters):
    '''
    updates theta by taking num_iters gradient steps with learning rate alpha
    :param X: input variables
    :param y: output variables
    :param theta: parameters
    :param alpha: learning rate
    :param num_iters: times of iteration
    :return: [theta, J_history]
    '''
    # Initialize some useful values
    m = len(y)
    J_history = np.zeros(num_iters)
    for i in range(num_iters):
        sum0, sum1 = 0, 0
        for j in range(m):
            sum0 = sum0 + (np.dot(X[j], theta) - y[j]) * X[j][0]
            sum1 = sum1 + (np.dot(X[j], theta) - y[j]) * X[j][1]
        theta[0] = theta[0] - alpha * sum0 / m
        theta[1] = theta[1] - alpha * sum1 / m

        # Save the cost J in every iteration
        J_history[i] = computeCost(X, y, theta)
    return theta, J_history


""" Part 2: Plotting """
print('Plotting Data ...')
data = np.loadtxt("ex1data1.txt", dtype=float, delimiter=',')
X = data[:, 0]
y = data[:, 1]
m = len(y)      # number of training examples

# Plot Data
fig0, ax0 = plt.subplots()
ax0.scatter(X, y, marker='x', label='Training data')
ax0.set_xlabel('Population of City in 10,000s')
ax0.set_ylabel('Profit in $10,000s')

""" Part 3: Cost and Gradient Descent """
X = np.c_[np.ones((m, 1)), data[:,0]]   # Add a column of ones to X
theta = np.zeros((2, 1))                # initialize fitting parameters

# Some gradient descent settings
iterations = 1500
alpha = 0.01

print('\nTesting the cost function ... \n')
# compute and display initial cost
J = computeCost(X, y, theta)
print('With theta = [0 ; 0], cost computed = ', J)
print('Expected cost value (approx) 32.07\n')

# further testing of the cost function
J = computeCost(X, y, np.array([[-1], [2]]))
print('With theta = [-1 ; 2], cost computed = ', J)
print('Expected cost value (approx) 54.24\n')

print('Running Gradient Descent ...\n')
# run gradient descent
theta, J_history = gradientDescent(X, y, theta, alpha, iterations)

# print theta to screen
print('Theta found by gradient descent: Œ∏0 =', theta[0], ', Œ∏1 =', theta[1])
print('Expected theta values (approx): -3.6303  1.6664\n')

# Plot the linear fit
ax0.plot(X[:, 1], np.dot(X, theta), color='r', label='Linear regression')
ax0.legend(loc='lower right')
ax0.set_title('Linear regression with one variable')

# Predict values for population sizes of 35,000 and 70,000
predict1 = np.dot([1, 3.5], theta)
predict2 = np.dot([1, 7], theta)
print('For population = 35,000, we predict a profit of ', predict1 * 10000)
print('For population = 70,000, we predict a profit of ', predict2 * 10000)

""" Part 4: Visualizing J(theta_0, theta_1) """
print('\nVisualizing J(theta_0, theta_1) ... \n')

# Plot the reducing of cost function during iteration
fig1, ax1 = plt.subplots()
ax1.plot(np.arange(iterations), J_history, 'b')
ax1.set_xlabel('Iterations')
ax1.set_ylabel(r'$J(\theta_0, \theta_1)$')
ax1.set_title(r'Reducing of $J(\theta_0, \theta_1)$ during iteration')

# Grid over which we will calculate J
theta0_vals = np.linspace(-10, 10, 100)
theta1_vals = np.linspace(-1, 4, 100)

# Initialize J_vals to a matrix of 0's
J_vals = np.zeros([len(theta0_vals), len(theta1_vals)])

# Fill out J_vals
for i in range(len(theta0_vals)):
    for j in range(len(theta1_vals)):
        t = [theta0_vals[i], theta1_vals[j]]
        J_vals[i][j] = computeCost(X, y, t)

x_contour, y_contour = theta0_vals, theta1_vals
theta0_vals, theta1_vals = np.meshgrid(theta0_vals, theta1_vals)

# Produce surface and contour plots of J(Œ∏)
fig2 = plt.figure()
ax2 = Axes3D(fig2)
ax2.plot_surface(theta0_vals, theta1_vals, J_vals.T, rstride=1, cstride=1, cmap=cm.rainbow)
ax2.set_xlabel(r'$\theta_0$')
ax2.set_ylabel(r'$\theta_1$')
ax2.set_title('Surface')

fig3, ax3 = plt.subplots()
CS = ax3.contour(theta0_vals, theta1_vals, J_vals.T, levels=50)
ax3.plot(theta[0], theta[1], 'bx')
ax3.set_xlabel(r'$\theta_0$')
ax3.set_ylabel(r'$\theta_1$')
ax3.set_title('Contour')

plt.show()
```

**Output:**

- Console

![image.png](https://i.loli.net/2020/03/03/uUr8wWJka7mXcHd.png)

- Training data with linear regression fit.

![image.png](https://i.loli.net/2020/03/03/3SVAbhH5y78zXkc.png)

- Reducing of $J(\theta_0, \theta_1)$ during iteration

![Figure_2.png](https://i.loli.net/2020/03/03/VWOMoKai5QcCAsn.png)

- Produce surface and contour plots of $J(Œ∏)$

![Figure_3.png](https://i.loli.net/2020/03/03/3HfaXsZnPyuelWr.png)

![Figure_4.png](https://i.loli.net/2020/03/03/wXyKdAUvBaHTWRY.png)

#### Ex1.2 Linear regression with multiple variables

**Instruction:**
In this part, you will implement linear regression with multiple variables to predict the prices of houses. Suppose you are selling your house and you want to know what a good market price would be. One way to do this is to first collect information on recent houses sold and make a model of housing prices.

**Code:**

```python
import numpy as np
import matplotlib.pyplot as plt

""" Part 0: Basic Function """
def computeCost(X, y, theta):
    '''
    Compute cost for linear regression
    :param X: input variables
    :param y: output variables
    :param theta: parameters
    :return: Cost function
    '''
    m = len(y)
    J = np.sum(np.square(X.dot(theta) - y)) / (2 * m)
    return J

def gradientDescentMulti(X, y, theta, alpha, num_iters):
    '''
    updates theta by taking num_iters gradient steps with learning rate alpha
    :param X: input variables
    :param y: output variables
    :param theta: parameters
    :param alpha: learning rate
    :param num_iters: times of iteration
    :return: [theta, J_history]
    '''
    # Initialize some useful values
    m, n = len(y), len(theta)
    J_history = np.zeros(num_iters)
    for i in range(num_iters):
        temp = np.dot((np.dot(X, theta) - y.reshape(m, 1)).T, X)
        theta = theta - alpha * temp.T / m
        J_history[i] = computeCost(X, y, theta)
    return theta, J_history

def featureNormalize(X):
    '''
    Normalize the features in X
    :param X: features
    :return: X_norm: normalized X; mean: mean value; sigma: standard deviation
    '''
    mean = np.mean(X, 0)
    sigma = np.std(X, 0)
    X_norm = (X - mean) / sigma

    return X_norm, mean, sigma

def normalEqn(X, y):
    '''
    Conputes the closed-form solution to linear regression
    :param X: input variables
    :param y: output variables
    :return: parameters
    '''
    theta = np.linalg.inv(X.T@X)@X.T@y
    return theta

""" Part 1: Feature Normalization """
# Load data
data = np.loadtxt('ex1data2.txt', dtype=float, delimiter=',')
X = data[:, 0:2]
y = data[:, 2:3]
m = len(y)

# Scale features and set them to zero mean
X_norm, mean, sigma = featureNormalize(X)
y_norm, _, _ = featureNormalize(y)

# Add intercept term to X
X_norm = np.c_[np.ones([m, 1]), X_norm]

""" Part 2: Gradient Descent """
print('Running gradient descent ... ')

# Choose some alpha value
alpha = 0.01
num_iters = 1000

# Init theta and run gradient descent
theta = np.zeros([3, 1])
theta, J_history = gradientDescentMulti(X_norm, y, theta, alpha, num_iters)

# Plot the convergence graph
fig1, ax1 = plt.subplots()
ax1.plot(np.arange(num_iters), J_history, 'b')
ax1.set_xlabel('Iterations')
ax1.set_ylabel(r'$J(\theta)$')
ax1.set_title(r'Convergence of $J(\theta)$')

# Display gradient descent's result
print('Theta computed from gradient descent: \n', theta)

""" Part 3: Normal Equations """
print('\nSolving with normal equations ... ')

# Add intercept term to X
X = np.c_[np.ones([m, 1]), X]

# Calculate the parameters from the normal equation
theta1 = normalEqn(X, y)
print('Theta computed from normal equations: \n', theta1)

plt.show()
```

**Output:**

- Console (There is some differences between the two results)

![image.png](https://i.loli.net/2020/03/04/HTaInqFUc7e9hPu.png)

- The convergence graph

![Figure_5.png](https://i.loli.net/2020/03/04/EDNHosUvmA8kw13.png)

